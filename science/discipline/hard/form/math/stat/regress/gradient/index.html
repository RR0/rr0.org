<!--#include virtual="/header-start.html" -->
<title>Descente de gradient</title>
<!--#include virtual="/header-end.html" -->
<section>
  <h2>Motivation</h2>
  <p>Trouver les valeurs de paramètres minimisant un <a href="../cost">coût</a>.
  </p>
</section>
<section>
  <h2>Analyse</h2>
  <figure class="right side">
    <figcaption>Descente de gradient vers un coût J minimal local (à droite) ou global (à gauche)</figcaption>
    <img src="gradient-descent.png" alt="Descente de gradient vers un coût J minimal local ou global"/>
  </figure>
  <p>L'<a
      href="/tech/info/soft/proj/impl/algo">algorithme</a> de descente de gradient consiste, si l'on considère :</p>
  <ul><li>une fonction hypothèse `h = θ_0 + θ_1x + ... θ_nx^n`</li><li>une <a
      href="../cost">fonction de coût</a> `J(θ)`</li></ul>
  <p>à changer (simultanément) les `θ_n` en fonction de la pente (<a
      href="/science/discipline/hard/form/math/ens/rel/func/deriv">dérivée</a>) induite par le coût :</p>
  <p><strong>Itérer</strong> jusqu'à convergence {<br/>
    &nbsp;&nbsp;`θ_j:=θ_j−α∂/(∂θ_j) J(θ)`<br/> }
  </p>
  <p>où `α` est le taux d'apprentissage (<i lang="en">learning rate</i>) à régler, déterminant la taille des "sauts"
    lors de la descente.</p>
  <p>Si l'on dispose de capacités de calcul matriciel rapides, on peut effectuer l'opération équivalente de manière
    <strong>vectorisée</strong>.</p>
</section>
<section>
  <h2>Notes</h2>
  <ul>
    <li>Sa complexité en `O(kn^2)` fait qu'elle est adaptée même si `n` est grand (contrairement à l'<a
        href="../normal">équation normale</a>).</li>
    <li>Si votre α est trop grand (votre saut trop rapide), vous pouvez vous retrouver à remonter ("sauter" de l'autre
      côté du trou) et donc diverger au lieu de converger.</li>
    <li>Pour être sûr de converger (ou comprendre pourquoi on ne converge pas), il peut être utile de tracer `J(θ)` et
      s'assurer qu'elle tend bien vers 0.</li>
    <li>Si des caractéristiques sont trop différentes (ayant une échelle ou un type différent), il pourra être
      nécessaire de les adapter :
      <ul>
        <li>Rééchelonnage (<i lang="en">feature scaling</i>) sur `0..1` : `x'=x/(max(x)-min(x))`</li>
        <li>Recentrage autour de la moyenne (<i lang="en">mean normalization</i>) : `x'=(x-bar x)/(max(x)-min(x))` ou de
          l'écart type : `x'=(x-bar x)/σ`</li>
      </ul>
    </li>
    <li>Afin d'éviter toutes erreurs de descente, on pourra aussi mettre en place une "vérification de gradient" (<i
        lang="en">gradient checking</i>) qui permettra de contrôler la valeur de la pente (<a
        href="/science/discipline/hard/form/math/ens/rel/func/deriv">dérivée</a>) trouvée. En effet si l'on considère un
      `ε` petit (`10^-4` par ex) : `d/(dθ) J(θ) ~~ (J(θ+ε)-J(θ-ε))/(2ε)`. Bien sûr cette vérification ne devra être
      utilisée que lors d'une phase de mise au point, puis désactivée une fois le gradient vérifié.</li>
    <li>D'autres algorithmes (plus complexes) d'optimisation évitent ces problèmes : un bon `α` est déterminé
      automatiquement et la convergence arrive donc rapidement :
      <ul>
        <li>Gradient conjugué</li>
        <li>BFGS</li>
        <li>L-BFGS</li>
      </ul>
    </li>
    <li>Une version plus performante (mais moins précise) est la descente de gradient <strong>stochastique</strong>, où
      l'on met à jour les paramètres en fonction d'un seul échantillon de données (x) à chaque fois, au lieu de tous.
    </li>
  </ul>
</section>
<section>
  <h2>Voir</h2>
  <ul>
    <li>Ruder, Sebastian: "<a href="https://ruder.io/optimizing-gradient-descent">An overview of gradient descent
      optimization algorithms</a>", 2016-01-16</li>
  </ul>
</section>
<!--#include virtual="/footer.html" -->
<style>.mjx-math * {
  line-height: 0;
}</style>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=AM_CHTML"></script>
