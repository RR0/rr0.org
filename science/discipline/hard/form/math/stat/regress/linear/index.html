<!--#include virtual="/header-start.html" -->
<title>Régression linéaire</title>
<style>.result {
  display: inline-block;
  margin: 0;
  padding: 0.5em;
  border: 1px solid red;
}</style>
<!--#include virtual="/header-end.html" -->
<section>
  <h2>Motivation</h2>
  <ul>
    <li>Déterminer si une variable quantitative (y) à une influence sur une autre (x)</li>
    <li><strong>Prédire</strong> un résultat (numérique) en fonction d'un ensemble de données connues</li>
  </ul>
</section>
<section>
  <h2>Analyse</h2>
  <p>La régression linéaire consiste à déterminer une fonction linéaire (et donc continue sur `RR`) capable de :</p>
  <ul>
    <li>reproduire des résultats passés</li>
    <li>prédire des résultats futurs</li>
  </ul>
  <figure class="right side">
    <figcaption>Exemple de régression linéaire à partir de points</figcaption>
    <img src="Linear_regression.svg" alt="Exemple de régression linéaire à partir de points"/>
  </figure>
  <p>Si l'on considère :</p>
  <ul>
    <li>`X` une matrice de `m xx n` caractéristiques (<i lang="en">features</i>), i.e. des lignes de données connues
    </li>
    <li>`y` un vecteur de `m` résultats connus</li>
  </ul>
  <p>Ce type de <a href="/science/discipline/hard/form/math/stat/regress">régression</a> consiste à déterminer la
    fonction <strong>hypothèse</strong> `h_Θ(x) = Θ_0 + x_1Θ_1 + x_2Θ_2 + ... + x_nΘ_n` dont les points sont les moins
    éloignés des valeurs réelles (connues du passé) pour espérer pouvoir en prédire des valeurs futures. Autrement dit
    déterminer les <strong>paramètres</strong> `Θ_0, Θ_1, ..., Θ_n` qui permettent à la fonction linéaire `h` d'avoir
    des points ayant une distance/différence minimale avec les points "réels" (connus).</p>
  <p>`Θ_0` qui ne dépend pas d'une variable, est une constante appelée <strong>biais</strong> (<i lang="en">bias</i>) ou
    <strong>intersection</strong> (de la fonction avec l'axe des ordonnées en `h_Θ(0)`).</p>
  <p>Les cas les plus courants ne requièrent cependant que 1 (`Θ_0 + xΘ_1`) ou 2 paramètres (`Θ_0 + x_1Θ_1 + x_2Θ_2`).
    S'il est besoin de représenter des fonctions plus complexes (typiquement si la fonction hypothèse ne correspond pas
    assez aux données ou en cas de <a
        href="/science/discipline/hard/form/math/stat/regress/underfit">surapprentissage</a>), on recourra aux <a
        href="../non-linear">régression non-linéaires</a>.
  </p>
</section>
<section>
  <h2>Conception</h2>
  <p>On cherche à trouver les `Θ` (`Θ_0, Θ_1`...) optimaux selon une méthode adaptée :</p>
  <ul>
    <li><strong>analytique</strong> via une <a href="../normal">équation normale</a> si `n` est petit (< 10000) et `X`
      inversible .</li>
    <li><strong>itérative</strong> autrement.</li>
  </ul>
  <p>La méthode itérative consiste à fournir à l'algorithme de <a
      href="/science/discipline/hard/form/math/stat/regress/gradient">descente de gradient</a> une <a
      href="../cost">fonction de <strong>coût</strong></a> `J(Θ)` permettant de calculer la différence entre `h` et `y`
    suivant de plus ou moins bons paramètres `Θ`.</p>
  <section>
    <h4>Coût</h4>
    <p>Comme les erreurs peuvent être positives ou négatives et que nous ne sommes intéressés que par l'écart/distance,
      on élève cette erreur au carré afin de le garantir toujours positif <span class="note">On pourrait théoriquement utiliser une fonction de valeur absolue à la place mais
        cela transformerait le résultat</span>. On annule/compense ensuite la mise au carré <span
          class="note">On parlera ici aussi de "fonction d'erreur carrée" (<i
          lang="en">squared error function</i>)</span> des erreurs en multipliant par 1/2 (ce qui annulera la <a
          href="/science/discipline/hard/form/math/ens/rel/func/deriv">dérivée</a> de `x^2` qui est `2x`) :
    </p>
    <p>`J(Θ) = 1/(2m) sum_(i=1)^m (h_Θ(x^((i))) - y^((i)))^2`</p>
    <p>ou en version vectorisée (en utilisant le calcul matriciel) :</p>
    <p>`J(Θ) = 1/(2m) (XΘ - vec y)^T (XΘ - vec y)`</p>
  </section>
  <section>
    <h4>Vectorisation</h4>
    <p>Afin de gagner en performance et simplicité d'écriture on peut réécrire la formule d'hypothèse sous forme de
      multiplication de matrices `Θ` et `x` (une ligne de `X`). Afin de conserver la constante `Θ_0`, on définira `x_0`
      = 1 :</p>
    <p>`Θ = [[Θ_0],[Θ_1],[Θ_2],[⋮],[Θ_n]]`, `x = [[x_0],[x_1],[x_2],[⋮],[x_n]]`</p>
    <p>Cependant on ne peut multiplier une matrice `1 xx (n + 1)` que par une matrice `(n + 1) xx 1`, et il faut donc
      transposer `Θ` pour obtenir le calcul attendu :</p>
    <p>`Θ^T = [Θ_0,Θ_1,Θ_2,...,Θ_n]`, `x = [[x_0],[x_1],[x_2],[⋮],[x_n]]`</p>
    <p>Ainsi :</p>
    <p>`h_Θ(x) = Θ^Tx`</p>
    <p>`h_Θ(x) = x_0Θ_0 + x_1Θ_1 + x_2Θ_2 + ... + x_nΘ_n` (le calcul recherché)</p>
    <p>Cependant au niveau de l'ensemble du <i lang="en">training set</i> `X` (ou <i lang="en">design matrix</i>), les
      `x^((i))` ne sont pas des vecteurs mais des lignes, de sorte qu'on doive plutôt y stocker des `(x^((i)))^T`s :
    </p>
    <p>`X = [[(x^((0)))^T],[(x^((1)))^T],[⋮],[(x^((m)))^T]]`</p>
    <p>Comme le vecteur `x` est déjà transposé de par sa notation en ligne, `Θ` ne doit plus l'être pour que leur
      multiplication s'opère. Cela reste équivalent car les 2 étant des vecteurs, `Θ^Tx = x^TΘ`.</p>
    <p>Le calcul recherché peut alors être effectué pour toutes les lignes de la matrice en multipliant `X` par le
      vecteur `Θ` :</p>
    <p>`h_Θ = [[(x^((0)))^TΘ],[(x^((1)))^TΘ],[⋮],[(x^((m)))^TΘ]]`</p>
    <p class="result">`h_Θ = XΘ`</p>
  </section>
</section>
<section>
  <h2>Notes</h2>
  <ul>
    <li>Avant tout travail de modélisation, une approche descriptive ou exploratoire est nécessaire pour dépister au
      plus tôt des difficultés dans les données : dissymétrie des distributions, valeurs atypiques, liaison non linéaire
      entre les variables.</li>
    <li>Le modèle suppose implicitement une notion préalable de <strong>causalité</strong> dans le sens où Y dépend de X
      car le modèle n’est pas symétrique (i.e. X ne dépend pas forcément de Y).</li>
  </ul>
</section>
<section>
  <h2>Exemples</h2>
  <p>En <a href="/tech/info/soft/data/science/ml">ML</a>, la régression linéaire permet de produire une fonction
    continue à partir de données connues, qui permet de prédire des valeurs probables pour d'autres valeurs de x ou y.
  </p>
</section>
<!--#include virtual="/footer.html" -->
<style>.mjx-math * {
  line-height: 0;
}</style>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=AM_CHTML"></script>
