<!--#include virtual="/header-start.html" -->
<title>Retro-propagation</title>
<!--#include virtual="/header-end.html" -->
<p><i lang="en">Back propagation</i>.</p>
<section>
  <h2>Motivation</h2>
  <p><a href="../deep">Apprendre</a> à partir de nombreux critères (trouver des hypothèses <a
      href="/science/discipline/hard/form/math/stat/regress/non-linear">non-linéaires</a> complexes).</p>
</section>
<section>
  <h2>Analyse</h2>
  <p>La rétro-propagation va consister à calculer l'<strong>erreur</strong> de prédiction pour adapter les paramètres en
    conséquence.</p>
</section>
<section>
  <h2>Conception</h2>
  <p>Ces erreurs de chaque couche `l` sont représentées par `δ^((l))` et sont accumulées dans `Δ^((l)) =
    Δ^((l))+δ^((l+1))(a^((l)))^T`.</p>
  <section>
    <h5>Coût</h5>
    <p>Pour calculer le coût pour un réseau neuronal, on généralise celui de la <a
        href="/science/discipline/hard/form/math/stat/regress/logistic">régression logistique</a> non pas pour une seule
      sortie y mais pour `K` sorties en insérant une somme supplémentaire pour tenir compte des `K` noeuds de sortie
      ainsi que des `S_l` unités (y compris le noeud de biais) de chaque couche :</p>
    <p>`J(Θ) = -1/m sum_(i=1)^m sum_(k=1)^K [y_k^((i)) log(h_θ(x^((i)))_k)+(1−y_k^((i)))log(1−h_θ(x^((i)))_k)] + λ/(2m)
      sum_(l=1)^(L-1) sum_(i=1)^(S_l) sum_(j=1)^(S_l+1) (θ_(j,i)^((l)))^2`</p>
    <p>Pour minimiser ce coût, nous allons chercher à minimiser la <a
        href="/science/discipline/hard/form/math/ens/rel/func/deriv">dérivée</a> de cette fonction (i.e. plus la pente
      du coût est faible, plus on s'approche de la solution).</p>
  </section>
</section>
<section>
  <h2>Exemples</h2>
</section>
<section>
  <h2>Notes</h2>
  <ul>
    <li>`θ` ne doit pas être initialisé à zéro (cela induirait une symmétrie qui empêcherait un entraînement correct),
      mais à des valeurs aléatoires non <égales></égales>.</li>
    <li>`J(θ)` n'est pas convexe dans ce cas et peut théoriquement mener à un minimum local au lieu du minimum global.
      Cependant dans la pratique les résultats sont généralement corrects.</li>
  </ul>
</section>
<section>
  <h2>Voir</h2>
  <ul>
    <li>Moawad, Assaad: "<a
        href="https://medium.com/datathings/neural-networks-and-backpropagation-explained-in-a-simple-way-f540a3611f5e">Neural
      networks and back-propagation explained in a simple way</a>", Medium, 2018-02-01</li>
  </ul>
</section>
<!--#include virtual="/footer.html" -->
<style>.mjx-math * {
  line-height: 0;
}</style>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=AM_CHTML"></script>
